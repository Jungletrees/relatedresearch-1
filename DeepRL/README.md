Deep Reinforcement Learning
=====


## 2018
---

### Palmer, et al.

[Lenient Multi-Agent Deep Reinforcement Learning](2018_Palmer.pdf)

  **A summary of Key breakthroughs/Approach Development** 

  [Palmer](https://cgi.csc.liv.ac.uk/~gpalmer/contact.php),[Tuyls](https://www.liverpool.ac.uk/computer-science/staff/karl-tuyls/), [Bloembergen](https://www.cwi.nl/people/daniel-bloembergen), and [Savani](https://www.liverpool.ac.uk/computer-science/staff/rahul-savani/) propose Multi-Agent Deep Reinforcement Learning approaches aimed at efficiently solving highly implicit coordination problems for parallel agents. The proposed approaches are based on two main deep reinforcement learning mechanisms: hysteretic Q-learning and leniency learning. The learning processes are made possible by convolutional neural networks:[Deep Q-Networks (DQNs](https://www.techopedia.com/definition/34032/deep-q-networks)from which agents/learners (hysteretic Q-learners and lenient learners) can update their state-action pairs using stored state transitions in the [Experience Replay Memories (ERMs)](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits).

  The authors develop and empirically evaluate a Lenient Deep Q-Network algorithm in which agents’ state-action pairs mapped in initially optimal temperature values update by decaying or cooling in every visit, therefore transitioning from an optimistic to an average reward learner in an environment that yields [Stochastic reward](https://en.wikipedia.org/wiki/Markov_reward_model).The authors make the leniency learning process more effective by introducing two extended processes: Retroactive Temperature Decay Schedule (TDS) and Greedy exploration strategy. TDS sets a sufficient temperature value at the terminal stages of failed learning processes to avoid repeated decay processes that may lead to the convergence to sub-optimal policy joints by ensuring that agents maintain a lenient disposition to one another. The original amount of leniency is determined by temperature values and is stored in Experience Replay Memories along with transition states. The proposed Lenient Multi-Agent Deep Q-Network architecture entails [autoencoders](https://towardsdatascience.com/deep-inside-autoencoders-7e41f319999f), which help in automatically clustering high dimensional or continuous state spaces using agents’ replay memories. Subsequently, Greedy exploration strategy enables agents to choose a greedy action within frequently visited states while remaining exploratory for less-frequented areas of the environment. The Greedy exploration strategy attains an average temperature of the current state of frequently visited state-action pairs while estimating the reward to the next action using less or unexplored state-action pairs, a process that ensures stability in the deep reinforcement learning process.
	
  According to the authors, the Hysteretic Q-learning process happen in the same way as leniency learning except in hysteretic Q-learning, state-action pairs are originally mapped to optimally predefined Q-values (estimates from the sum of discounted future rewards stored in multi-layer neural networks) stored in agents’ ERMs. The Hysteretic Deep Q-Networks facilitate the [Hysteretic Q-learning process](https://www.semanticscholar.org/paper/Hysteretic-q-learning-%3Aan-algorithm-for-learning-in-Matignon-Laurent/ce8ad8b3dd21ff33c79f249ce85ac5856592a913)in two learning rates: one called alpha where every state-action interaction increases the value estimate (Q-value) and a Beta learning rate where every interaction reduces the value estimate. However, as a result of the Beta learning rate interdependencies with other agents’ exploration strategies, the Hysteretic Q-learning process is most likely to lead to sub-optimal policy joint in environments with stochastic rewards. As a result, the authors proposed and empirically evaluated an improved Hysteretic Deep Q-learning Network algorithm with a pre-computed learning rate schedule called the Scheduled-HDQN, which applies less optimism towards state transitions near terminal states compared to earlier transitions within the episode to avoid overestimation of Q-values in a Stochastic reward environment. Both Leniency and Hysteretic Q-learning approaches use [Double-Deep Q-Networks](https://towardsdatascience.com/double-deep-q-networks-905dd8325412)to induce sufficient optimism in the early learning stages to allow learning agents to converge to an optimal joint policy. The Double-Deep Q-Networks entail current and target networks whereby current state weight values and resulting action-state weight values after n transitions are stored respectively, forming a double Q-learning process. 
	
  **Emperical Validation/Why the developed approach is superior** 
	   
  The authors empirically evaluate each developed approach using the Coordinated Multi-Agent Object Transportation Problem (CMOTP), which is a fully-cooperative multi-agent problem requiring reinforcement learning agents to learn fully-cooperative joint-policies from processing high dimensional and noisy image observations.  To effectively test the distributed problem-solving capacities as well as compare efficiencies between the proposed approaches: Lenient Deep Q-Network (LDQN) algorithm and Schedules-Hysteretic Deep Q-Network, the authors introduces two extensions to CMOTP environment.  The first extension is the narrow passage provision, which allows the testing of lenient agents’ ability to prevent the premature decay of temperature values. The second extension introduces two dropzones yielding stochastic rewards, which aim at testing agents’ ability to converge towards an optimal joint-policy while receiving misleading rewards. The authors compare the developed approaches’ efficiencies in converging to optimal joint policies in standard CMOTP, deterministic CMOTP, and stochastic reward CMOTP, in noisy and non-noisy conditions. They find that LDQNs significantly outperform standard and scheduled-HDQNs within environments that yield stochastic rewards. After performing a LDQN hyperparameter analysis, the authors make a novel finding that the highest performing agents within stochastic reward domains use a steep temperature decay schedule that maintains high temperatures for early transitions combined with a temperature modification coefficient that slows down the transition from optimist to average reward learner, and an exploration exponent that delays the transition from explorer to exploiter. Another notable novel breakthrough by the authors is the introduction of two extensions to leniency, including a retroactive temperature decay schedule that prevents the premature decay of temperatures for state-action pairs and a Greedy exploration strategy that encourages agents to remain exploratory in states with a high average temperature value. The extensions can, in theory, also be used by lenient agents within non-deep settings.
	
  **What to look out for/Pending validations** 
	
  Having successfully merged leniency with a Double-DQN architecture and demonstrated that leniency can help Multi-Agent Deep Reinforcement Learning agents solve a challenging fully cooperative CMOTP using high-dimensional and noisy images as observations, the proposed approach is left to be tested and validated using more sophisticated components like Prioritized Experience Replay Memory and more implicit coordination tasks like the ones having randomly distributed stochastic rewards throughout episodes. 
	
  **Application**
	
  The proposed deep reinforcement learning approach: Lenient Multi-Agent Deep Reinforcement Learning would be novel in solving highly coordinated problems in large tournament stochastic games, stock market predictions, betting odds, manufacturing robotics processes, and human robot cooperation. 
